Action（五子棋）：
棋盘大小 10 * 10
采用强化学习（策略价值网络），用AI训练五子棋AI
请说明都有哪些模块，不同模块的原理

主要使用了MCTS和UCB算法,还有神经网络
强化学习在这里主要由两个部分组成，一个部分是环境(environment），另一个部分是策略（policy）。环境由三个部分组成（状态(state)，动作(action)，奖励（reward））通俗点来讲，环境就是一个黑箱函数，该函数的输出为当前的state和上一个action的reward，而接受的输入为action。用围棋来举例子就是，围棋当前棋盘上的棋子的位置就是状态，而我们选择下了一步棋，那么棋盘的状态就发生了改变（多了一个字），我们之前选择下的那步棋的好坏就是我们的reward。而策略（policy）是在这里抽象为一个输入状态，输出action的函数。policy比较类似人类的思考过程，棋手（policy）通过观察棋盘（state），下了一步棋（做出action）。
所以强化学习就可以理解为寻找一个输入状态输出动作，来使得我们的环境反馈的reward最大的一个函数。
假如说五子棋的棋盘大小为15 x 15大小，那么在遍历的情况下一共有225种可能。而且五子棋的胜负只有在一局棋结束的时候才能判别，在这之前我们很难衡量一步棋的好坏。这就需要我们的蒙特卡洛树搜索（Monte Carlo Tree Search）算法了
蒙特卡罗树搜索本质上是一颗有不同节点（node）的树，节点与节点之间相连接。每个节点可以在这里可以代表一个棋盘的状态，假设我们的棋盘大小为15*15，而初始棋盘（棋盘上什么都没有的状态）的状态就是我们的最开始的根节点状态，而其下理论上有225个子节点，分别代表了初始玩家下在225个不同位置时的棋盘状态，而这225个字节点，每个子结点其下理论上又单独可有最多224的子节点，以此类推。对于每个节点，上面储存着该节点的访问次数（counter）与每一个子节点的Q值（每一个子节点相对于父节点来说代表着一个走子的动作{action}，而Q值在这里可以简要理解为该动作好坏的评分）。
首先在我们的训练过程中，模型每次下出一步棋，会有两个流程。一个流程是模拟（simulation），一个是实际走子（play out）。模拟过程可以理解为，在我们正式走子之前，进行的预测（simulation），根据我们的预测结果来进行实际走子（play out）
模拟过程一般会进行很多次，我们首先对其中一次进行讲解。对于传统的蒙特卡罗树搜索来说，首先基于选取一个“最优”动作.我们持续不断的选取“最优”动作，直到我们来到一个节点，并选择了一个我们之前从来每访问的动作，也因此这里并没有一个与该动作对应的节点。到这里我们完成了图中selection的部分。然后进入expansion部分，我们在这里创建新节点，记住，每次模拟只创建一个节点。然后进入MCTS的Simulation部分（注意这里的Simulation与上文中提到的模拟Simulation并不是同一个，做好区分）传统的MCTS会进行随机策略，抽象到棋盘上就相当于在棋盘上面随机走子。随机走子走到了尽头，我们就来到了Backpropagation的部分，我们将随机走子的结果（访问次数，胜负之类的信息）更新到这次模拟所经历的所有节点中，也就是说递归更新所有子节点的父节点。（对于一个节点来说，其执行一个动作会前进到其下的节点，其下的节点就是子节点，而相对于子节点来说，该节点就是父节点。
我们这里使用的MCTS与传统的MCTS有所不同，首先在selection阶段，我们选择节点是根据神经网络指导下进行选择的，每次选择选取最大的upper confidence bound值的节点.在这里UCB是一个权衡置信度（或者方差）和探索值的公式，通俗点来说，就是未探索的action和已经探索很多次但是探索反馈很高的action都会有较大的UCB值。Q值代表蒙特卡罗树中以探索的值，Q值的更新就是在之前蒙特卡罗树搜索中每一次backpropagation反馈的值的平均值。而P值是先验概率，是由神经网络计算得到的值，而N是该action的探索次数，而c是一个用于调节平衡模型探索的超参数。所以这个公式的特点就是，对于一个反复探索同时有很好的奖励的action，和缺乏探索的action都会有很高的值。这样就能很好的平衡探索与最优之间的平衡了。
其次的不同点是，在MCTS的backpropagation阶段，如果当前节点并没有分出胜负，是由神经网络进行打分（value），反馈的是value而不是随机探索出来的结果。具体value是什么我们放在神经网络阶段继续讲解。

MCTS的流程理解：
对于任意的局面State（就是节点），要么被展开过（expand），要么没有展开过（就是叶子节点）
展开过的节点可以通过Select步骤进入下一个局面State，下一个局面State仍然是这个过程……，一直持续下去直到这盘棋分出胜负，或者遇到某个局面没有被展开过为止
如果没有展开过，那么执行expand操作，通过神经网络得到每个动作的概率和胜率v，把这些动作添加到树上，最后把胜率v回传（backed up）
Expand的同时就在做evaluate，这是为Backup做准备，因为在Backup步骤，我们要用v来更新Q值的，但是如果只做了一次Select，棋局还没有结束，此时无法得到v，必须要等到一盘棋下完才可以得到v（当前这步的好坏评估是由最终结果决定）

走子：当模拟过程结束后，我们进入了走子过程。在这一步我们用不同action的探索次数之间的比例当作我们实际的概率分布，我们按照该概率分布选择我们的action（概率可以增加一点噪音做一些额外的exploration）。这里的概率分布要保存下来，训练神经网络的时候要用。我们按照 模拟 - 走子 - 模拟 - 走子的流程一直走下去，直到我们的游戏结束。游戏结束的时候我们将游戏记录保存下来，可以用于神经网络的训练。
然后是神经网络:输入为当前棋盘，输出为双端口，分别表示当前棋盘的状态值（value）和当前棋盘各个位置的走子的概率，这里的棋盘状态值，即机器评价的当前局面的“好坏”,论文里面用的是13层Resnet,但实际可以根据个人偏好更改.

Policy Gradients：
基本思想，直接根据状态输出动作的概率 => 使用神经网络
作用，输入当前的状态，神经网络输出在这个状态下采取每个动作的概率
对于强化学习来说，我们不知道动作的正确与否，只能通过奖励值来判断这个动作的相对好坏
如果一个动作得到的reward多，那么就让它出现的概率增大，如果一个动作得到的reward少，就让它出现的概率减小
loss= -log(prob)*vt
log(prob)表示状态s对动作a的吃惊程度，如果概率越小，-log(prob)越大，vt表示当前状态s下采取动作a所能得到的奖励（即当前的奖励和未来奖励的贴现值的求和）
Policy Gradients算法：
蒙特卡洛策略梯度reinforce算法, 使用价值函数v(s)来近似
输入：N个蒙特卡洛完整序列,训练步长α
输出：策略函数的参数θ
for 每个蒙特卡洛序列:
  a. 用蒙特卡洛法计算序列每个时间位置t的状态价值vt
  b. 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数θ：

返回策略函数的参数θ
这里的策略函数可以是softmax策略，高斯策略或者其他策略

