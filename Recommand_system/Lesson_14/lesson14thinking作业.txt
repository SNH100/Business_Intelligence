Thinking1:机器学习中的监督学习、非监督学习、强化学习有何区别

监督学习（Supervised learning）：
监督学习即具有特征（feature）和标签（label）的，即使数据是没有标签的，也可以通过学习特征和标签之间的关系，判断出标签——分类。
简言之：提供数据，预测标签。比如对动物猫和狗的图片进行预测，预测label为cat或者dog。
通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类
无监督学习（Unsupervised learning）：
无监督学习即只有特征，没有标签，只有特征，没有标签的训练数据集中，通过数据之间的内在联系和相似性将他们分成若干类——聚类。根据数据本身的特性，从数据中根据某种度量学习出一些特性。
eg.比如一个人没有见过恐龙和鲨鱼，如果给他看了大量的恐龙和鲨鱼，虽然他没有恐龙和鲨鱼的概念，但是他能够观察出每个物种的共性和两个物种间的区别的，并对这两种动物予以区分。
简言之：给出数据，寻找隐藏的关系。
半监督学习（Semi-Supervised learning）：
半监督学习使用的数据，一部分是标记过的，而大部分是没有标记的，和监督学习相比较，半监督学习的成本较低，但是又能达到较高的准确度，即综合利用有类标的和没有类标的数据，来生成合适的分类函数。
简言之：少部分标记，大部分未知
强化学习（Reinforcement learning）：
强化学习与半监督学习类似，均使用未标记的数据，但是强化学习通过算法学习是否距离目标越来越近，我理解为激励与惩罚函数。
简言之：通过不断激励与惩罚，达到最终目的。
区别：
（1）监督学习有反馈，无监督学习无反馈，强化学习是执行多步之后才反馈。
（2）强化学习的目标与监督学习的目标不一样，即强化学习看重的是行为序列下的长期收益，而监督学习往往关注的是和标签或已知输出的误差。
（3）强化学习的奖惩概念是没有正确或错误之分的，而监督学习标签就是正确的，并且强化学习是一个学习+决策的过程，有和环境交互的能力（交互的结果以惩罚的形式返回），而监督学习不具备。

Thinking2:什么是策略网络，价值网络，有何区别?

策略网络：
任何游戏，玩家的输入被认为是行为a，每个输入（行为）导致一个不同的输出，这些输出称为游戏的状态s.对于给定的输入，通过学习给出一个确定输出的网络：
（动作1，状态1），（动作2，状态2）

价值网络（数值网络）：
通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络
奖励更多的状态，会在数值网络中的数值Value更大
这里的奖励是奖励期望值，我们会从状态集合中选择最优的

区别:
策略网络的输出，是一个落子的概率分布;价值网络的输出，一个可能获胜的数值，即“价值”，这个价值训练是一种回归(regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测
对于价值网络，当前局面的价值=对终局的估计,结果是一个标量


Thinking3: 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的

简单蒙特卡罗搜索基于一个强化学习模型Mv和一个模拟策略π.在此基础上，对于当前我们要选择动作的状态St, 对每一个可能采样的动作a∈A,都进行K轮采样，这样每个动作a都会得到K组经历完整的状态序列(episode)。
MCTS摒弃了简单蒙特卡罗搜索里面对当前状态St每个动作都要进行K次模拟采样的做法，而是总共对当前状态St进行K次采样，这样采样到的动作只是动作全集A中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。

第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。

第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。

第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。

第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录


Thinking4：假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑

可以采用美团使用的MDP模型,其中几个基础的强化学习要素需要考虑
推荐系统看作智能体（Agent），用户看作环境（Environment），推荐系统与用户的多轮交互过程可以建模为MDP：
State：Agent对Environment的观测，即用户的意图和所处场景。
Action：以List-Wise粒度对推荐列表做调整，考虑长期收益对当前决策的影响。
Reward：根据用户反馈给予Agent相应的奖励，为业务目标直接负责。
P(s,a)：Agent在当前State s下采取Action a的状态转移概率

Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路

强化学习被认为是推动策略学习的一个有前途的方向。然而,在实际环境中进行自动驾驶车辆的强化学习训练涉及到难以负担的试错。更可取的做法是先在虚拟环境中训练，然后再迁移到真实环境中。
自动驾驶的目标是使车辆感知它的环境和在没有人参与下的行驶。实现这个目标最重要的任务是学习根据观察到的环境自动输出方向盘、油门、刹车等控制信号的驾驶策略。
传统的强化学习解决方案异步优势Actor-Arbitor（A3C）来训练自动驾驶汽车，这种方法在多种机器学习任务中表现的很出色。A3C算法是将几种经典的强化学习算法与异步并行线程思想相结合的一种基本的行动Actor-Critic。多个线程与环境的无关副本同时运行，生成它们自己的训练样本序列。这些Actor-learners继续运行,好像他们正在探索未知空间的不同部分。对于一个线程，参数在学习迭代之前同步,完成后更新。
