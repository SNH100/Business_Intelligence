###Thinking1：奇异值分解SVD的原理是怎样的，都有哪些应用场景

SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵A的SVD为：
A=UΣ(V)T.其中U 是一个m × m 的矩阵，Σ是一个m × n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个n × n的矩阵。U和V 都是酉矩阵，即满足U^{T} U = I ,V^{T}V=I 
如果我们将A的转置和A做矩阵乘法，那么会得到n × n的一个方阵A^{T}A.既然A^{T}A是方阵,那么就可以进行特征分解,特征分解得到的n个张量张成的n×n矩阵就是V;同理,我们将A和A的转置做矩阵乘法可以得到U,
A=UΣ(V)T implies AV=UΣV^{T}V implies AV=UΣ Avi =σi ui 这样我们可以求出每个奇异值,进而求出奇异值矩阵Σ. A=UΣV^{T} implies A^{T}=VΣ^{T}U^{T}
implies A^{T}A=VΣ^2V^{T}. 这样我们可以得到特征值矩阵等于奇异值矩阵的平方.
对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。

根据以上的奇异值的性质,我们可以把SVD用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。也可以用于压缩,比如图像压缩等等

###Thinking2: funkSVD, BiasSVD，SVD++算法之间的区别是怎样的

funkSVD算法避开传统SVD的稀疏问题，而且只用两个矩阵进行相乘,损失函数=P和Q矩阵乘积得到的评分，与实际用户评分之差.
BiasSVD算法考虑到了用户和商品的偏好bias,将与个性化无关的部分，设置为偏好(Bias)部分,损失函数=P和Q矩阵乘积得到的评分加上用户平均评分与用户以及商品的偏好，与实际用户评分之差,并对P,Q和用户,商品的偏好做过拟合处理.
SVD++算法在BiasSVD算法基础上进行了改进，考虑用户的隐式反馈,如用户的点击,浏览等行为.目标函数就是在BiasSVD的基础上再减去隐式反馈item集合的平方根乘以Q的转置以及用户i所有的隐式反馈修正值之和的积,并额外对用户i所有的隐式反馈修正值之和做过拟合处理.

###Thinking3: 矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足

矩阵分解在推荐系统中可以把评分矩阵分解为商品和用户两个维度,对于这个 用户-物品 矩阵，可以利用非空项的数据来预测空白项的数据，即预测用户对于其未接触到的物品的评分，并根据预测情况，将评分高的物品推荐给用户。
不足在于它的泛化能力,只能考虑到2个维度,对于大于2的多个维度的情况时,就不能采用矩阵分解的方式进行处理.

###Thinking4：假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的

方法：计算当前看过的这部小说特征向量与整个小说特征矩阵的余弦相似度，取相似度最大的Top-k个.余弦相似度:通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
similarity=cos(sigma)=a*b/(a模*b模)
步骤:
Step1，对小说描述（Desc）进行特征提取
N-Gram，提取N个连续字的集合，作为特征
TF-IDF，按照(min_df, max_df)提取关键词，并生成TFIDF矩阵
Step2，计算小说之间的相似度矩阵余弦相似度
Step3，对于看过的这部小说，选择相似度最大的Top-K个小说进行输出

###Thinking5：Word2Vec的应用场景有哪些

Word2vec主要是应用于NLP的,比如机器翻译,语言建模,聊天机器人,问题回答等场景;但是它也可以用在商品推荐上,比如根据用户的已做的一系列点击商品的行为,对用户的下一次点击做商品推荐.
